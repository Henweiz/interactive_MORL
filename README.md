# Interactive MORL

## Abstract

Optimizing multiple objectives in various real-life problems can be challenging, especially when the optimized solution must align with the user's preferences. Over the past decade, extensive research has been conducted on applying Reinforcement Learning to Multi-Objective Optimization (MORL) problems. The majority of the methods focus on providing a dense Pareto Front as a result. The issues are that generating a large solution set requires high computational costs, and it can still be difficult for the user to find their most preferred solutions from the solution set. In this research, we propose an \textit{interactive} MORL method where the user shares their current preferred solution, and the algorithm utilizes this information to enhance its learning process to find preference-aligned solutions. This is achieved by bounding the solution space to only search for new policies that outperform the previously user-selected solution. We evaluate our method using an artificial user function to simulate preferences, comparing it with other MORL methods and Interactive Multi-Objective Optimization methods. Metrics include the number of learning steps required to converge to a preferred solution and the value achieved on the artificial user function. The results demonstrate a higher overall preference and faster convergence when the user has a strong preference for one objective over another. 
